{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_batch(batch_size: int,\n",
    "              speed: float=None,\n",
    "              chw: tuple[int, int]=(3, 20, 20),\n",
    "              seq_len: int=10, ) -> torch.Tensor:\n",
    "    if speed is None:\n",
    "        speed = torch.randint(0, chw[2], (batch_size, )).float() / (seq_len + 1)\n",
    "\n",
    "    all = torch.zeros(batch_size, seq_len + 1, *chw)\n",
    "\n",
    "    # Create a line and move it from left to right\n",
    "    for b in range(batch_size):\n",
    "        # Generate a random starting position for the line\n",
    "        start_pos = torch.randint(0, chw[2] - int(speed[b] * seq_len), (1,)).item()\n",
    "\n",
    "        for t in range(seq_len + 1):\n",
    "            # Calculate the current position of the line\n",
    "            current_pos = start_pos + int(speed[b] * (t + 1))\n",
    "            current_pos = min(max(current_pos, 0), chw[2] - 1)  # Ensure line stays within bounds\n",
    "\n",
    "            # Fill in the pixels of the line\n",
    "            all[b, t, :, :, current_pos] = 1  # Assuming single channel\n",
    "\n",
    "    x = all[:, :-1, :, :, :]\n",
    "    y = all[:, 1:, :, :, :]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Test the function\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x, y = get_batch(batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "example_x, example_y = x[0], y[0]\n",
    "# plot each of the 10 frames using matplotlib\n",
    "fig, axs = plt.subplots(2, seq_len, figsize=(20, 4))\n",
    "for i in range(seq_len):\n",
    "    axs[0, i].imshow(example_x[i, 0], cmap='gray')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[0, i].set_title(f'x[{i}]')\n",
    "    axs[1, i].imshow(example_y[i, 0], cmap='gray')\n",
    "    axs[1, i].axis('off')\n",
    "    axs[1, i].set_title(f'y[{i}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 16, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"Implementation based on the paper: Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting (https://arxiv.org/abs/1506.04214).\n",
    "    \n",
    "    A change made to the original implementation is the W_ci, W_co, and W_cf biases. The original implementation uses a 3D scalar value for the gates, but this meant a parameters for each element in the cell state, which is too much.\n",
    "    To mitigate this, I will increase the internal dimensions of the LSTM convolutional layers, but make the gates convolutional layers as well. \n",
    "    The intuition is that the model will be able to introduce more information in the channel dimension which will help the gates decide on which information to keep and which to discard.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_shape: tuple[int, int, int],\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 activation: str='tanh'):\n",
    "        \"\"\"The constructor of the ConvLSTM cell. The cell is supposed to act similarly to the default nn.LSTM cell, but with convolutional layers instead of linear layers.\n",
    "        \n",
    "        :param in_shape: The shape of the input tensor (in_channels, height, width).\n",
    "        :param out_channels: The number of output channels of the cell.\n",
    "        :param kernel_size: The size of the convolutional kernel.\n",
    "        :param activation: The activation function of the cell. Can be either 'relu' or 'tanh'.\n",
    "        \"\"\"\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert activation in [\"relu\", \"tanh\"], \"Activation must be either 'relu' or 'tanh'.\"\n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd.\"\n",
    "\n",
    "        if activation == \"tanh\": self.activation = torch.tanh \n",
    "        elif activation == \"relu\": self.activation = torch.relu\n",
    "\n",
    "        # Single convolutional layer will do all calculations in parallel\n",
    "        self.conv = nn.Conv2d(in_channels=in_shape[0] + out_channels, \n",
    "                              out_channels=4 * out_channels, \n",
    "                              kernel_size=kernel_size, \n",
    "                              padding=kernel_size // 2)\n",
    "\n",
    "        # The gates\n",
    "        self.W_ci = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.W_co = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.W_cf = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h_prev: torch.Tensor, c_prev: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"The forward pass of the ConvLSTM cell. The ConvLSTM is supposed to act similarly to the \n",
    "        default nn.LSTM cell, but with convolutional layers instead of linear layers.\n",
    "\n",
    "        :param x: The input tensor of shape (batch_size, in_channels, height, width).\n",
    "        :param h_prev: The hidden state of the previous time step. (batch_size, out_channels, height, width)\n",
    "        :param c_prev: The cell state of the previous time step. (batch_size, out_channels, height, width)\n",
    "\n",
    "        :return: (ht, ct) The cell state and the hidden state of the current time step.\n",
    "            - ht is of shape (batch_size, out_channels, height, width)\n",
    "            - ct is of shape (batch_size, out_channels, height, width)\n",
    "        \"\"\"\n",
    "        # Concatenate the input tensor with the hidden state along the channel dimension\n",
    "        xh = torch.cat([x, h_prev], dim=1) # (batch_size, in_channels + out_channels, height, width)\n",
    "        # Put the concatenated tensor through the convolutional layer\n",
    "        # NOTE: The output of the convolutional layer is split into 4 parts: input gate, forget gate, cell state, output gate (the output channels of the conv is 4 * out_channels)\n",
    "        i_conv, f_conv, C_conv, o_conv = torch.chunk(self.conv(xh), chunks=4, dim=1) # each element is of shape (batch_size, out_channels, height, width)\n",
    "\n",
    "        it = torch.sigmoid(i_conv + self.W_ci(c_prev)) # (batch_size, out_channels, height, width)\n",
    "        ft = torch.sigmoid(f_conv + self.W_cf(c_prev)) # (batch_size, out_channels, height, width)\n",
    "        ct = ft * c_prev + it * self.activation(C_conv) # (batch_size, out_channels, height, width)\n",
    "        ot = torch.sigmoid(o_conv + self.W_co(ct)) # (batch_size, out_channels, height, width)\n",
    "        ht = ot * self.activation(ct) # (batch_size, out_channels, height, width)\n",
    "\n",
    "        return ht, ct\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_shape: tuple[int, int, int], \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 activation: str='tanh'):\n",
    "        \"\"\"The constructor of the ConvLSTM layer. The layer is supposed to act similarly to the default nn.LSTM cell, but with convolutional layers instead of linear layers.\n",
    "        \n",
    "        :param in_shape: The shape of the input tensor (in_channels, height, width).\n",
    "        :param out_channels: The number of output channels of the cell.\n",
    "        :param kernel_size: The size of the convolutional kernel.\n",
    "        :param activation: The activation function of the cell. Can be either 'relu' or 'tanh'.\n",
    "        \"\"\"\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.cell = ConvLSTMCell(in_shape, out_channels, kernel_size, activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h0c0: tuple[torch.Tensor, torch.Tensor]=None) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"The forward pass of the ConvLSTM cell. The ConvLSTM is supposed to act similarly to the PyTorch nn.LSTM layer, but with convolutional layers instead of linear layers.\n",
    "        \n",
    "        :param x: The input tensor of shape (batch_size, seq_len, in_channels, height, width).\n",
    "        :param h0c0: The initial hidden and cell states. Both are of shape (batch_size, out_channels, height, width). If None, the initial states are set to zeros.\n",
    "\n",
    "        :return: output, (h_n, c_n) where:\n",
    "            - output: The output tensor of shape (batch_size, seq_len, out_channels, height, width).\n",
    "            - h_n: The hidden state of the last time step. (batch_size, out_channels, height, width).\n",
    "            - c_n: The cell state of the last time step. (batch_size, out_channels, height, width).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _, height, width = x.shape\n",
    "        if h0c0 is None:\n",
    "            h0 = torch.zeros(batch_size, self.out_channels, height, width)\n",
    "            c0 = torch.zeros(batch_size, self.out_channels, height, width)\n",
    "        else:\n",
    "            h0, c0 = h0c0\n",
    "        \n",
    "        h = h0\n",
    "        c = c0\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.cell(x[:, t], h, c)\n",
    "            output.append(h)\n",
    "        \n",
    "        return torch.stack(output, dim=1), (h, c)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, out_channels: int, input_shape: tuple[int, int, int]):\n",
    "        super(Model, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.lstm = ConvLSTM(input_shape, hidden_dim, 3)\n",
    "        self.conv = nn.Conv2d(hidden_dim, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        h, _ = self.lstm(x) # (batch_size, seq_len, hidden_dim, height, width)\n",
    "        # NOTE: PyTorch doesn't accept the shape (batch_size, seq_len, channels, height, width) for the convolutional layer, so we need to reshape the tensor\n",
    "        h = h.view(-1, *h.shape[2:])\n",
    "        y = self.conv(h).view(x.shape[0], x.shape[1], self.out_channels, x.shape[-2], x.shape[-1])\n",
    "\n",
    "        if target is not None:\n",
    "            loss = nn.MSELoss()(y, target)\n",
    "            return y, loss\n",
    "        else: \n",
    "            return y, None\n",
    "        \n",
    "x, _ = get_batch(1)\n",
    "model = Model(16, 3, x.shape[2:])\n",
    "y, loss = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Number of parameters (3, 1280, 1280): 245855753\n",
      "After Number of parameters (3, 1280, 1280):  163403\n"
     ]
    }
   ],
   "source": [
    "model = Model(50, 3, (3, 1280, 1280))\n",
    "print(\"Before Number of parameters (3, 1280, 1280): 245855753\")\n",
    "print(f\"After Number of parameters (3, 1280, 1280):  {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters (3, 20, 20): 105523\n"
     ]
    }
   ],
   "source": [
    "model = Model(40, 3, (3, 20, 20))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "print(f\"Number of parameters (3, 20, 20): {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37b18d42ace49418ba29a87b980c8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n",
      "torch.Size([16, 3, 20, 20]) torch.Size([16, 40, 20, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m x, y \u001b[38;5;241m=\u001b[39m get_batch(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, chw\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m), seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m y_pred, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 118\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, target)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 118\u001b[0m     h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, seq_len, hidden_dim, height, width)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# NOTE: PyTorch doesn't accept the shape (batch_size, seq_len, channels, height, width) for the convolutional layer, so we need to reshape the tensor\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mh\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[1;34m(self, x, h0c0)\u001b[0m\n\u001b[0;32m    103\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m--> 105\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), (h, c)\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mConvLSTMCell.forward\u001b[1;34m(self, x, h_prev, c_prev)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Put the concatenated tensor through the convolutional layer\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# NOTE: The output of the convolutional layer is split into 4 parts: input gate, forget gate, cell state, output gate (the output channels of the conv is 4 * out_channels)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m i_conv, f_conv, C_conv, o_conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(xh), chunks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# each element is of shape (batch_size, out_channels, height, width)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m it \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(i_conv \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_prev\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# (batch_size, out_channels, height, width)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(f_conv \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_cf(c_prev)) \u001b[38;5;66;03m# (batch_size, out_channels, height, width)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m ct \u001b[38;5;241m=\u001b[39m ft \u001b[38;5;241m*\u001b[39m c_prev \u001b[38;5;241m+\u001b[39m it \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(C_conv) \u001b[38;5;66;03m# (batch_size, out_channels, height, width)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "total_loss = 0\n",
    "\n",
    "for i in tqdm(range(5000)):\n",
    "    x, y = get_batch(batch_size=16, chw=(3, 20, 20), seq_len=10)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    y_pred, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    if i % 500 == 499:\n",
    "        print(f\"Epoch {i} - Loss: {total_loss / 100}\")\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 40, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 40, 20, 20])\n",
      "torch.Size([1, 3, 20, 20]) torch.Size([1, 40, 20, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'GT3')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hc = None\n",
    "\n",
    "data, y = get_batch(1, speed=[1.4], chw=(3, 20, 20), seq_len=10)\n",
    "with torch.no_grad():\n",
    "    x = data[0, 0].unsqueeze(0).unsqueeze(0)\n",
    "    gt1 = y[0, 0].clone()\n",
    "\n",
    "    inter, hc = model.lstm(x, hc)\n",
    "    y1 = model.conv(inter.view(1, *inter.shape[2:]))\n",
    "\n",
    "    x = data[0, 1].unsqueeze(0).unsqueeze(0)\n",
    "    gt2 = y[0, 1].clone()\n",
    "\n",
    "    inter, hc = model.lstm(x, hc)\n",
    "    y2 = model.conv(inter.view(1, *inter.shape[2:]))\n",
    "\n",
    "    x = data[0, 2].unsqueeze(0).unsqueeze(0)\n",
    "    gt3 = y[0, 2].clone()\n",
    "\n",
    "    inter, hc = model.lstm(x, hc)\n",
    "    y3 = model.conv(inter.view(1, *inter.shape[2:]))\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "vmin, vmax = -0.1, 1.1\n",
    "axs[0, 0].imshow(y1.squeeze()[0], vmin=vmin, vmax=vmax)\n",
    "axs[0, 0].set_title(\"Y1\")\n",
    "axs[0, 1].imshow(y2.squeeze()[0], vmin=vmin, vmax=vmax)\n",
    "axs[0, 1].set_title(\"Y2\")\n",
    "axs[0, 2].imshow(y3.squeeze()[0], vmin=vmin, vmax=vmax)\n",
    "axs[0, 2].set_title(\"Y3\")\n",
    "axs[1, 0].imshow(gt1.squeeze()[0], vmin=vmin, vmax=vmax)\n",
    "axs[1, 0].set_title(\"GT1\")\n",
    "axs[1, 1].imshow(gt2.squeeze()[0], vmin=vmin, vmax=vmax)\n",
    "axs[1, 1].set_title(\"GT2\")\n",
    "axs[1, 2].imshow(gt3.squeeze()[0], vmin=vmin, vmax=vmax)\n",
    "axs[1, 2].set_title(\"GT3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
